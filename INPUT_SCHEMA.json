{
    "title": "Spreadsheet import input",
    "type": "object",
    "schemaVersion": 1,
    "required": [],
    "properties": {
        "datasetIds": {
            "title": "Dataset IDs",
            "type": "array",
            "description": "Datasets that should be deduplicated and merged",
            "editor": "stringList"
        },
        "fields": {
            "title": "Fields for deduplication",
            "type": "array",
            "description": "Fields whose combination should be unique for the item to be considered unique. If none are provided, the actor does not perform deduplication.",
            "editor": "stringList"
        },
        "output": {
            "title": "What to output",
            "type": "string",
            "description": "What will be pushed to the dataset from this actor",
            "editor": "select",
            "default": "unique-items",
            "enum": ["unique-items", "duplicate-items", "nothing"],
            "enumTitles": ["Unique items", "Duplicate items", "Nothing (checking number of duplicates)"]
        },
        "mode": {
            "title": "Mode",
            "type": "string",
            "description": "How the loading and deduplication process will work.",
            "editor": "select",
            "default": "dedup-after-load",
            "enum": ["dedup-after-load", "dedup-as-loading"],
            "enumTitles": ["Dedup after load (high memory use, keeps items ordered as in original datasets)", "Dedup as loading (low memory, doesn't keep items ordered as in original datasets)"]
        },
        "outputDatasetId": {
            "title": "Output dataset ID or name (optional)",
            "type": "string",
            "description": "Optionally can push into dataset of your choice. If you provide a dataset name that doesn't exist, a new named dataset will be created.",
            "editor": "textfield"
        },
        "fieldsToLoad": {
            "title": "Limit fields to load",
            "type": "array",
            "description": "You can choose which fields to load only. Useful to speed up the loading and reduce memory needs.",
            "editor": "stringList",
            "sectionCaption": "Limit fields to load for faster execution",
            "sectionDescription": "You can only load certain fields which can make this actor several times faster. Useful for checking data quality and duplicates. If you need to push data in the original format, don't use this."   
        },
        "preDedupTransformFunction": {
            "title": "Pre dedup transform function",
            "type": "string",
            "description": "Function to transform items before deduplication is applied. For 'dedup-after-load' mode this is done for all items at once. For 'dedup-as-loading' this is applied to each batch separately.",
            "editor": "javascript",
            "prefill": "async (items, { Apify }) => {\n    return items;\n}",
            "sectionCaption": "Transforming functions",
            "sectionDescription": "Optionally transform the data. Filling just one function is enough but you can use both for 2 process transform."   
        },
        "postDedupTransformFunction": {
            "title": "Post dedup transform function",
            "type": "string",
            "description": "Function to transform items after deduplication is applied. For 'dedup-after-load' mode this is done for all items at once. For 'dedup-as-loading' this is applied to each batch separately.",
            "editor": "javascript",
            "prefill": "async (items, { Apify }) => {\n    return items;\n}"
        },
        "actorOrTaskId": {
            "title": "Actor or Task ID (or name)",
            "type": "string",
            "description": "Use Actor or Task ID (e.g. `nwua9Gu5YrADL7ZDj`) or full name (e.g. `apify/instagram-scraper`).",
            "editor": "textfield",
            "sectionCaption": "Load datasets from actor or task"
        },
        "onlyRunsNewerThan": {
            "title": "Only runs newer than",
            "type": "string",
            "description": "Use a date format of either `YYYY-MM-DD` or with time `YYYY-MM-DDTHH:mm:ss`.",
            "editor": "datepicker"
        },
        "onlyRunsOlderThan": {
            "title": "Only runs older than",
            "type": "string",
            "description": "Use a date format of either `YYYY-MM-DD` or with time `YYYY-MM-DDTHH:mm:ss`.",
            "editor": "datepicker"
        },
        "outputTo": {
            "title": "Where to output",
            "type": "string",
            "description": "Either can output to a single dataset or to split data into KV records depending on upload batch size. KV is upload is much faster but data end up in many files.",
            "editor": "select",
            "default": "dataset",
            "enum": ["dataset", "key-value-store"],
            "enumTitles": ["Dataset", "Key-Value Store (split into records)"],
            "sectionCaption": "Advanced config",
            "sectionDescription": "You should not need to touch these unless for very specific needs."   
        },
        "parallelLoads": {
            "title": "Parallel loads",
            "type": "integer",
            "description": "Datasets can be loaded in parallel batches to speed things up if needed.",
            "default": 10,
            "maximum": 100
        },
        "parallelPushes": {
            "title": "Parallel pushes",
            "type": "integer",
            "description": "Deduped data can be pushed in parallel batches to speed things up if needed. If you want the data to be in the exact same order, you need to set this to 1.",
            "default": 5,
            "minimum": 1,
            "maximum": 50
        },
        "uploadBatchSize": {
            "title": "Upload batch size",
            "type": "integer",
            "description": "How many items it should upload in one pushData call. Useful to not overload Apify API. Only important for dataset upload.",
            "default": 500,
            "minimum": 10,
            "maximum": 1000
        },
        "batchSizeLoad": {
            "title": "Download batch size",
            "type": "integer",
            "description": "How many items it will load in a single batch.",
            "default": 50000
        },
        "offset": {
            "title": "Offset (how many items to skip from start)",
            "type": "integer",
            "description": "By default we don't skip any items which is the same as setting offset to 0. For multiple datasets, it takes offset into the sum of their item counts but that is not very useful."
        },
        "limit": {
            "title": "Limit (how many items to load)",
            "type": "integer",
            "description": "By default we don't limit the number loaded items"
        },
        "verboseLog": {
            "title": "verbose log",
            "type": "boolean",
            "description": "Good for smaller runs. Large runs might run out of log space.",
            "default": false
        },
        "nullAsUnique": {
            "title": "Null fields are unique",
            "type": "boolean",
            "description": "If you want to treat null (or missing) fields as always unique items.",
            "default": false
        },
        "datasetIdsOfFilterItems": {
            "title": "Dataset IDs for just deduping",
            "type": "array",
            "description": "The items from these datasets will be just used as a dedup filter for the main datasets. These items are loaded first and then the main datasets are compared for uniqueness and pushed.",
            "editor": "stringList"
        },
        "customInputData": {
            "title": "Custom input data",
            "type": "object",
            "prefill": {},
            "description": "You can pass custom data as a JSON object to be accessible in the transform functions as part of the 2nd parameter object.",
            "editor": "json"
        },
        "appendDatasetIds": {
            "title": "Append dataset IDs to items",
            "type": "boolean",
            "description": "Useful for transform functions. Each item will get a field `__datasetId__` with the dataset ID it came from.",
            "default": false
        }
    }
}
